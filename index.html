<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="FunSeg: Closed-Loop Vision-Language Functional Segmentation for Task-Driven 3D Scene Understanding">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>FunSeg: Closed-Loop Vision-Language Functional Segmentation</title>

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">

  <style>
    body {
      font-family: 'Noto Sans', sans-serif;
      background-color: #ffffff;
      color: #4a4a4a;
      line-height: 1.6;
      margin: 0;
      padding: 0;
    }

    h1, h2, h3 {
      font-family: 'Google Sans', sans-serif;
      color: #363636;
      margin-bottom: 0.5em;
    }

    h1 {
      font-size: 2.5rem;
      font-weight: 700;
      line-height: 1.2;
      margin-top: 40px;
    }

    h2 {
      font-size: 1.75rem;
      font-weight: 600;
      margin-top: 50px;
      border-bottom: 2px solid #f1f1f1;
      padding-bottom: 10px;
      text-align: center;
    }

    h3 {
      font-size: 1.3rem;
      font-weight: 600;
      margin-top: 30px;
      color: #555;
    }

    .container {
      max-width: 960px;
      margin: 0 auto;
      padding: 0 20px;
    }

    /* Header & Authors */
    .header-content {
      text-align: center;
      padding: 50px 0 30px;
    }
    .authors { font-size: 1.2rem; margin-top: 20px; }
    .authors a { color: #3273dc; text-decoration: none; }
    .affiliations { font-size: 1rem; color: #666; margin-top: 10px; }

    /* Buttons */
    .link-buttons {
      margin-top: 25px;
      display: flex;
      justify-content: center;
      gap: 15px;
      flex-wrap: wrap;
    }
    .btn {
      display: inline-flex;
      align-items: center;
      background-color: #363636;
      color: #fff;
      padding: 10px 20px;
      border-radius: 30px;
      text-decoration: none;
      font-weight: 600;
      font-size: 0.95rem;
      transition: background-color 0.3s ease;
    }
    .btn:hover { background-color: #555; }
    .btn i { margin-right: 8px; }

    /* Abstract */
    .abstract-box {
      background-color: #f5f5f5;
      padding: 30px;
      border-radius: 10px;
      margin-top: 30px;
      text-align: justify;
    }
    .abstract-title { font-weight: 700; font-size: 1.5rem; text-align: center; margin-bottom: 15px; }

    /* Figures */
    .figure-container {
      text-align: center;
      margin: 40px 0;
    }
    .figure-container img {
      max-width: 100%;
      height: auto;
      border-radius: 8px;
      margin-bottom: 10px;
    }
    .caption {
      font-size: 0.95rem;
      color: #666;
      font-style: italic;
      text-align: center;
      max-width: 850px;
      margin: 0 auto;
    }

    /* Two Column Layout for Images */
    .columns {
      display: flex;
      gap: 20px;
      margin: 20px 0;
    }
    .column {
      flex: 1;
    }
    .column img {
      width: 100%;
      border-radius: 8px;
      box-shadow: 0 4px 8px rgba(0,0,0,0.1);
    }

    /* Table */
    table {
      width: 100%;
      border-collapse: collapse;
      margin: 30px 0;
      font-size: 0.9rem;
      font-family: 'Google Sans', sans-serif;
    }
    th, td { padding: 12px 10px; text-align: center; }
    thead tr { border-top: 2px solid #000; border-bottom: 1px solid #000; }
    tbody tr { border-bottom: 1px solid #e0e0e0; }
    tbody tr:last-of-type { border-bottom: 2px solid #000; }
    .highlight-row { background-color: #e8f5e9; font-weight: bold; }

    /* Footer & Bibtex */
    pre {
      background-color: #f5f5f5;
      padding: 20px;
      border-radius: 8px;
      overflow-x: auto;
      font-family: 'Consolas', 'Monaco', monospace;
      font-size: 0.85rem;
    }
    footer {
      margin-top: 60px;
      padding: 30px 0;
      background-color: #f5f5f5;
      text-align: center;
      font-size: 0.9rem;
      color: #888;
    }
    /* Contributions List */
    ul.contributions {
      list-style-type: none; /* Remove default bullets */
      padding: 0;
      max-width: 800px;
      margin: 0 auto;
    }

    ul.contributions li {
      position: relative;
      padding-left: 30px;
      margin-bottom: 15px;
    }

    ul.contributions li::before {
      content: "ðŸš€"; /* Custom bullet */
      position: absolute;
      left: 0;
      top: 0;
    }
    @media (max-width: 768px) {
      .columns { flex-direction: column; }
    }
  </style>
</head>
<body>

<div class="container">

  <!-- Header -->
  <div class="header-content">
    <h1>FunSeg: Closed-Loop Vision-Language Functional Segmentation<br>for Task-Driven 3D Scenes</h1>
    
    <div class="authors">
      <span>Jingwei Zhang<sup>1</sup></span>,
      <span>Co-Author Name<sup>2</sup></span>,
      <span>Co-Author Name<sup>1</sup></span>
    </div>
    <div class="affiliations">
      <sup>1</sup>University of Chinese Academy of Science, Institute of Automation, Chinese Academy of Sciences(CASIA) &nbsp;&nbsp; <sup>2</sup>Another Institution
    </div>

    <div class="link-buttons">
      <a href="#" class="btn"><i class="fas fa-file-pdf"></i> arXiv</a>
      <a href="https://github.com/reasimei/funseg" class="btn"><i class="fab fa-github"></i> Code</a>
      <a href="https://scenefun3d.github.io/documentation/" class="btn"><i class="fas fa-database"></i> Dataset</a>
    </div>
  </div>

  <!-- Teaser Image -->
  <!-- ä½¿ç”¨åŽŸå›¾ image (2).png -->
  <div class="figure-container">
    <img src="figures/fig1.png" loading="lazy" alt="Teaser Comparison">
    <p>
      <strong>FunSeg</strong> is a powerful framework designed for <strong>understanding and segmenting functionality in 3D</strong> scenes. Given a high-resolution 3D scene, a set of RGBD views displaying the scene, and a instruction of the action to be performed, FunSeg can segment functional objects that can be used to perform specific actions, especially for instructions requiring <strong>fine-grained spatial understanding</strong>, where its performance is superior to other strategies.
    </p>
  </div>

  <!-- Abstract -->
  <div class="abstract-box">
    <div class="abstract-title">Abstract</div>
    <p>
      Interacting with 3D scenes requires precise localization of fine-grained functional elements such as handles and switches. Existing open-vocabulary 3D perception pipelines predominantly operate in an open-loop manner, making them vulnerable to semantic ambiguity and error propagation.
    </p>
    <p>
      We present <strong>FunSeg</strong>, a closed-loop vision-language framework that enables robust task-driven 3D functional segmentation. By introducing <strong>Spatial-Sequential Instruction Tuning</strong>, we equip vision-language models with fine-grained ordinal and spatial reasoning. Furthermore, a <strong>VLM-driven feedback mechanism</strong> actively verifies 2D cues before 3D lifting, transforming fragile open-loop inference into a self-verifying process. FunSeg achieves state-of-the-art performance on the SceneFun3D benchmark.
    </p>
  </div>

  <!-- Contributions -->
  <h2>Key Contributions</h2>
  <ul class="contributions">
    <li><strong>Closed-Loop Framework:</strong> A novel pipeline that feeds back verification signals to correct segmentation errors.</li>
    <li><strong>Spatial-Sequential Tuning:</strong> Empowering VLMs with the ability to understand "second from left" or "top right" through targeted fine-tuning.</li>
    <li><strong>SOTA Performance:</strong> Significant improvements on the SceneFun3D benchmark compared to existing open-vocabulary methods.</li>
  </ul>

  <!-- Method Section -->
  <h2>Methodology</h2>

  <!-- Pipeline Figure -->
  <!-- ä½¿ç”¨åŽŸå›¾ fig2.png -->
  <div class="figure-container">
    <h3>System Architecture</h3>
    <img src="figures/fig2.png" loading="lazy" alt="FunSeg Architecture">
    <div class="caption">
      Figure 1: <strong>Overview of the FunSeg framework.</strong> The system consists of an offline Spatial-Sequential Tuning stage to enhance VLM sensitivity to position and order, and an online Close-loop Inference stage where the VLM actively verifies segmentation proposals.
    </div>
  </div>

  <!-- Details: Spatial & Prompting -->
  <div class="figure-container">
    <h3>Spatial-Sequential Tuning & Data</h3>
    <!-- ä½¿ç”¨åŽŸå›¾ image (4).png -->
    <img src="figures/fig6.png" loading="lazy" alt="Spatial Concepts">
    <div class="caption">
      Figure 2: <strong>Spatial-Sequential Data.</strong> We fine-tune the model on fine-grained spatial attributes (e.g., "second", "top left", "third") to resolve ambiguities in complex multi-object scenes.
    </div>
  </div>

  <!-- Results Section -->
  <h2>Experimental Results</h2>

  <!-- Quantitative Table -->
  <div style="overflow-x: auto;">
    <table>
      <caption>Table 1: Quantitative comparison on SceneFun3D benchmark.</caption>
      <thead>
        <tr>
          <th>Method</th>
          <th>mAP</th>
          <th>AP<sub>50</sub></th>
          <th>AP<sub>25</sub></th>
          <th>mAR</th>
          <th>AR<sub>50</sub></th>
          <th>AR<sub>25</sub></th>
          <th>mIoU</th>
        </tr>
      </thead>
      <tbody>
        <tr><td>OpenMask3D</td><td>0.20</td><td>0.20</td><td>0.40</td><td>20.3</td><td>24.5</td><td>27.0</td><td>0.20</td></tr>
        <tr><td>OpenIns3D</td><td>0.00</td><td>0.00</td><td>0.00</td><td>40.5</td><td>46.7</td><td>51.5</td><td>0.10</td></tr>
        <tr><td>LERF</td><td>0.00</td><td>0.00</td><td>0.00</td><td>34.2</td><td>35.1</td><td>36.0</td><td>0.00</td></tr>
        <tr><td>Fun3DU</td><td>6.16</td><td>11.69</td><td>21.80</td><td>27.37</td><td>36.18</td><td>41.57</td><td>10.72</td></tr>
        <tr class="highlight-row">
          <td>FunSeg (Ours)</td>
          <td>10.85</td>
          <td>23.15</td>
          <td>40.67</td>
          <td>26.63</td>
          <td>39.55</td>
          <td>50.34</td>
          <td>17.82</td>
        </tr>
      </tbody>
    </table>
  </div>


  <!-- Main Qualitative Results -->
  <!-- ä½¿ç”¨åŽŸå›¾ fig7.png -->
  <div class="figure-container">
    <h3>VLM Effectiveness</h3>
    <img src="figures/fig7.png" loading="lazy" alt="Qualitative Results Grid">
    <div class="caption">
      Figure 3: <strong> Comparative analysis.</strong> The base VLM fails to recognize the "second drawer". Our model, enhanced by Spatial-Sequential Instruction Tuning, correctly parses fine-grained hierarchies and spatial relations.
    </div>
  </div>

  <!-- Visual Metrics Comparison -->
  <!-- ä½¿ç”¨åŽŸå›¾ image (6).png -->
  <div class="figure-container">
    <h3>Segmentation Results</h3>
    <p>
      <span style="display:inline-block;width:10px;height:10px;background:blue;margin-right:6px;"></span>
      Ground Truth;
      <span style="display:inline-block;width:10px;height:10px;background:red;margin:0 6px 0 12px;"></span>
      Prediction;
      <span style="display:inline-block;width:10px;height:10px;background:green;margin:0 6px 0 12px;"></span>
      Overlap.
    </p>
    <img src="figures/fig8.png" loading="lazy" alt="Visual Metrics Comparison">
    <div class="caption">
      Figure 4: <strong> Visualizing 3D functional grounding performance on SceneFun3D.</strong> Compared
to the training-free baseline Fun3DU, our method, FunSeg, demonstrates superior accuracy in resolving complex spatial prepositions(line 1,2) and grounding small-scale
functional elements (e.g., handles, plugs)(line 3,4). While the baseline often fails due
to over-segmentation or semantic hallucinations, FunSeg leverages spatial-sequential
tuning to maintain high Precision and IoU. 
    </div>
  </div>
  
  <div class="figure-container">
    <h3>Generalizability</h3>
    <p>
      Demonstrate the model's performance on non-SceneFun3D data (the following pictures were taken at the author's home) 
      to show that the model learns general spatial logic rather than dataset-specific distributions.
    </p>
    <img src="figures/fig9.png" loading="lazy" alt="Qualitative Results Grid">
    <div class="caption">
    </div>
  </div>
  <div class="figure-container">
    <img src="figures/fig10.png" loading="lazy" alt="Qualitative Results Grid">
    <div class="caption">

    </div>
  </div>
  <div class="figure-container">
    <img src="figures/fig11.png" loading="lazy" alt="Qualitative Results Grid">
    <div class="caption">

    </div>
  </div>
  <div class="figure-container">
    <img src="figures/fig12.png" loading="lazy" alt="Qualitative Results Grid">
    <div class="caption">

    </div>
  </div>
  <div class="figure-container">
    <img src="figures/fig13.png" loading="lazy" alt="Qualitative Results Grid">
    <div class="caption">

    </div>
  </div>
  <div class="figure-container">
    <img src="figures/fig14.png" loading="lazy" alt="Qualitative Results Grid">
    <div class="caption">

    </div>
  </div>

  <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title">Citation</h2>
If you find FunSeg useful for your work, please cite:
    <pre><code>@article{funseg2026,
  title   = {FunSeg: Closed-Loop Vision-Language Functional Segmentation for Task-Driven 3D Scenes},
  author  = {Zhang, Jingwei and ...},
  journal = {arXiv preprint},
  year    = {2026}
}</code></pre>
      </div>
    </section>  

  <footer>
    <p>This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.</p>
  </footer>

</div>

</body>
</html>